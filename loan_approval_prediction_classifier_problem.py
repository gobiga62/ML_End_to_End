# -*- coding: utf-8 -*-
"""Loan Approval prediction - Classifier problemipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pm-p4RrdA3ddohmlgzMbV0LbWKlofxlz

**Loan** **Approval** **Prediction**

Importing relevant libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings("ignore")

"""Loading the dataset and creating dataframe"""

train_df=pd.read_csv('loan_sanction_train.csv')
test_df=pd.read_csv('loan_sanction_test.csv')

train_df.head()

test_df.head()

train_df.shape

#Check for missing values
train_df.isnull().sum().sort_values(ascending=False)

#Check for duplicate data
duplicate_data=train_df.duplicated().any()
print(duplicate_data)

# Observing the summarized information of data
train_df.info()

#If the no. of unique values are < 20 then the variable is categorical otherwise continuous
train_df.nunique()

#Bar charts for categorical variables
def PlotBarCharts(inp, columns):
    fig, subPlot=plt.subplots(nrows=1, ncols=len(columns), figsize=(25,5))
    fig.suptitle('Bar charts for categorical variables')

    for col, plot in zip(columns, range(len(columns))):
        inp.groupby(col).size().plot(kind='bar',ax=subPlot[plot])

# Calling the function
PlotBarCharts(inp=train_df, columns=['Gender','Married','Dependents','Education','Self_Employed','Loan_Amount_Term','Credit_History','Property_Area','Loan_Status'])

train_df[['ApplicantIncome','CoapplicantIncome','LoanAmount']].describe(include='all')

#Distribution of numerical variable
sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 2, figsize=(10, 8))

sns.histplot(data=train_df, x="ApplicantIncome", kde=True, ax=axs[0, 0], color='skyblue')
sns.histplot(data=train_df, x="CoapplicantIncome", kde=True, ax=axs[0, 1], color='orange')
sns.histplot(data=train_df, x="LoanAmount", kde=True, ax=axs[1, 0], color='green');

#outliers
sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 2, figsize=(10, 8))

sns.boxplot(data=train_df, x="ApplicantIncome",  ax=axs[0, 0], color='skyblue')
sns.boxplot(data=train_df, x="CoapplicantIncome", ax=axs[0, 1], color='orange')
sns.boxplot(data=train_df, x="LoanAmount",  ax=axs[1, 0], color='green');

train_df=train_df.drop(['Loan_ID'],axis = 1)
train_df.head()

#Check for missing values
train_df.isnull().sum().sort_values(ascending=False)

#Replacing missing values with mode for categorical variables
train_df['Gender'].fillna(train_df['Gender'].mode()[0],inplace=True)
train_df['Married'].fillna(train_df['Married'].mode()[0],inplace=True)
train_df['Dependents'].fillna(train_df['Dependents'].mode()[0],inplace=True)
train_df['Self_Employed'].fillna(train_df['Self_Employed'].mode()[0],inplace=True)
train_df['Credit_History'].fillna(train_df['Credit_History'].mode()[0],inplace=True)
train_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].mode()[0],inplace=True)

#Check for missing values
train_df.isnull().sum().sort_values(ascending=False)

train_df['LoanAmount'].fillna(train_df['LoanAmount'].mean(),inplace=True)

#Check for missing values
train_df.isnull().sum().sort_values(ascending=False)

# Converting the binary nominal variable to numeric
#train_df['Gender'].replace({'Male':1, 'Female':0}, inplace=True)
#train_df['Married'].replace({'Yes':1, 'No':0}, inplace=True)
#train_df['Education'].replace({'Graduate':1, 'Not Graduate':0}, inplace=True)
#train_df['Self_Employed'].replace({'Yes':1, 'No':0}, inplace=True)
#train_df['Loan_Status'].replace({'Y':1, 'N':0}, inplace=True)

def ordinal_encoding(data=train_df, column_name='Dependents'):
  # Create a dictionary to map category to its order
  category_order = {cat: i for i, cat in enumerate(sorted(train_df['Dependents'].unique()))}
  # Apply the order mapping to the column
  train_df['Dependents']=train_df['Dependents'].replace(category_order)
  return train_df

# Assuming your data is in a pandas DataFrame named 'data'
train_df = ordinal_encoding(train_df.copy(), 'column1')  # Apply to each ordinal column

train_df.head()

train_df['Dependents'].unique()

#Correlation heatmap
numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']

# Select the numerical columns from the DataFrame
num_cols = train_df[numerical_columns]

# Calculate the correlation matrix
correlation_matrix = num_cols.corr()
dataplot = sns.heatmap(num_cols.corr(), cmap="YlGnBu", annot=True)
sns.set(rc = {'figure.figsize':(10,8)})
plt.show()

def iqr_outlier_removal(df, numerical_columns):
  df_filtered = train_df.copy()  # Avoid modifying the original DataFrame
  for col in numerical_columns:
    if not pd.api.types.is_numeric_dtype(df[col]):
      raise TypeError(f"Column '{col}' is not numerical.")  # Raise error for non-numerical columns
    Q1 = df_filtered[col].quantile(0.25)
    Q3 = df_filtered[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_filtered = df_filtered.loc[(df_filtered[col] >= lower_bound) & (df_filtered[col] <= upper_bound)]
  return df_filtered

# Sample DataFrame (replace with your actual data)
data = train_df
df = pd.DataFrame(data)

# Specify the numerical columns for outlier removal
numerical_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']

# Filter outliers
df_filtered = iqr_outlier_removal(df.copy(), numerical_cols)

# Print the filtered DataFrame
print(df_filtered)

df_filtered = pd.get_dummies(df_filtered)

# Drop columns
df_filtered = df_filtered.drop(['Gender_Female', 'Married_No', 'Education_Not Graduate',
              'Self_Employed_No', 'Loan_Status_N'], axis = 1)

# Rename columns name
new = {'Gender_Male': 'Gender', 'Married_Yes': 'Married',
       'Education_Graduate': 'Education', 'Self_Employed_Yes': 'Self_Employed',
       'Loan_Status_Y': 'Loan_Status'}

df_filtered.rename(columns=new, inplace=True)

# Sqrt Transformation

df_filtered.ApplicantIncome = np.sqrt(df_filtered.ApplicantIncome)
df_filtered.CoapplicantIncome = np.sqrt(df_filtered.CoapplicantIncome)
df_filtered.LoanAmount = np.sqrt(df_filtered.LoanAmount)

#Distribution of numerical variable after outlier treatment
sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 2, figsize=(10, 8))

sns.histplot(data=df_filtered, x="ApplicantIncome", kde=True, ax=axs[0, 0], color='skyblue')
sns.histplot(data=df_filtered, x="CoapplicantIncome", kde=True, ax=axs[0, 1], color='orange')
sns.histplot(data=df_filtered, x="LoanAmount", kde=True, ax=axs[1, 0], color='green');

#Check outliers after applying transformation
sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 2, figsize=(10, 8))

sns.boxplot(data=df_filtered, x="ApplicantIncome",  ax=axs[0, 0], color='skyblue')
sns.boxplot(data=df_filtered, x="CoapplicantIncome", ax=axs[0, 1], color='orange')
sns.boxplot(data=df_filtered, x="LoanAmount",  ax=axs[1, 0], color='green');

X = df_filtered.drop(["Loan_Status"], axis=1)
y = df_filtered["Loan_Status"]

#Resampling imbalanced dataset
from imblearn.over_sampling import SMOTE
X, y = SMOTE().fit_resample(X, y)

plt.figure(figsize=(10, 8))
sns.countplot(y=y, data=df_filtered, palette="coolwarm")
plt.ylabel('Loan Status')
plt.xlabel('Total')
plt.show()

#Normalization
from sklearn.preprocessing import MinMaxScaler
X = MinMaxScaler().fit_transform(X)

#Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

import os
import scipy

from scipy import stats
from scipy.stats import pearsonr
from scipy.stats import ttest_ind
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import CategoricalNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

#Logistic Regression
LRclassifier = LogisticRegression(solver='saga', max_iter=500, random_state=1)
LRclassifier.fit(X_train, y_train)

y_pred = LRclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
LRAcc = accuracy_score(y_pred,y_test)
print('LR accuracy: {:.2f}%'.format(LRAcc*100))

!pip install interpret

from interpret.glassbox import (LogisticRegression,
                                ClassificationTree,
                                ExplainableBoostingClassifier)
from interpret import show

scoreListknn = []
for i in range(1,21):
    KNclassifier = KNeighborsClassifier(n_neighbors = i)
    KNclassifier.fit(X_train, y_train)
    scoreListknn.append(KNclassifier.score(X_test, y_test))

plt.figure(figsize=(10, 8))
plt.plot(range(1,21), scoreListknn)
plt.xticks(np.arange(1,21,1))
plt.xlabel("K value")
plt.ylabel("Score")
plt.show()
KNAcc = max(scoreListknn)
print("KNN best accuracy: {:.2f}%".format(KNAcc*100))

SVCclassifier = SVC(kernel='rbf', max_iter=500)
SVCclassifier.fit(X_train, y_train)

y_pred = SVCclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
SVCAcc = accuracy_score(y_pred,y_test)
print('SVC accuracy: {:.2f}%'.format(SVCAcc*100))

NBclassifier1 = CategoricalNB()
NBclassifier1.fit(X_train, y_train)

y_pred = NBclassifier1.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
NBAcc1 = accuracy_score(y_pred,y_test)
print('Categorical Naive Bayes accuracy: {:.2f}%'.format(NBAcc1*100))

scoreListDT = []
for i in range(2,21):
    DTclassifier = DecisionTreeClassifier(max_leaf_nodes=i)
    DTclassifier.fit(X_train, y_train)
    scoreListDT.append(DTclassifier.score(X_test, y_test))

plt.figure(figsize=(10, 8))
plt.plot(range(2,21), scoreListDT)
plt.xticks(np.arange(2,21,1))
plt.xlabel("Leaf")
plt.ylabel("Score")
plt.show()
DTAcc = max(scoreListDT)
print("Decision Tree Accuracy: {:.2f}%".format(DTAcc*100))

from sklearn.metrics import f1_score, accuracy_score
tree = ClassificationTree()
tree.fit(X_train, y_train)
print("Training finished.")
y_pred = tree.predict(X_test)
print(f"F1 Score {f1_score(y_test, y_pred, average='macro')}")
print(f"Accuracy {accuracy_score(y_test, y_pred)}")

!pip install shap

import shap

ex = shap.KernelExplainer(DTclassifier.predict, shap.sample(X_train,100))

shap.initjs()
shap_values = ex.shap_values(X_test)
shap.summary_plot(shap_values, X_test)

import pandas as pd
shap.initjs()

# Convert X_test to a Pandas DataFrame
X_test_df = pd.DataFrame(X_test)

# Access the 10th row of the DataFrame
shap_values = ex.shap_values(X_test_df.loc[10])

# Create a force plot
shap.force_plot(ex.expected_value, shap_values, X_test_df.loc[10])

scoreListRF = []
for i in range(2,25):
    RFclassifier = RandomForestClassifier(n_estimators = 1000, random_state = 1, max_leaf_nodes=i)
    RFclassifier.fit(X_train, y_train)
    scoreListRF.append(RFclassifier.score(X_test, y_test))

plt.figure(figsize=(10, 8))
plt.plot(range(2,25), scoreListRF)
plt.xticks(np.arange(2,25,1))
plt.xlabel("RF Value")
plt.ylabel("Score")
plt.show()
RFAcc = max(scoreListRF)
print("Random Forest Accuracy:  {:.2f}%".format(RFAcc*100))

paramsGB={'n_estimators':[100,200,300,400,500],
      'max_depth':[1,2,3,4,5],
      'subsample':[0.5,1],
      'max_leaf_nodes':[2,5,10,20,30,40,50]}

GB = RandomizedSearchCV(GradientBoostingClassifier(), paramsGB, cv=20)
GB.fit(X_train, y_train)

print(GB.best_estimator_)
print(GB.best_score_)
print(GB.best_params_)
print(GB.best_index_)

GBclassifier = GradientBoostingClassifier(subsample=1, n_estimators=200, max_depth=4, max_leaf_nodes=10)
GBclassifier.fit(X_train, y_train)

y_pred = GBclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
GBAcc = accuracy_score(y_pred,y_test)
print('Gradient Boosting accuracy: {:.2f}%'.format(GBAcc*100))

compare = pd.DataFrame({'Model': ['Logistic Regression', 'K Neighbors',
                                  'SVM', 'Categorical NB',
                                   'Decision Tree',
                                  'Random Forest', 'Gradient Boost'],
                        'Accuracy': [LRAcc*100, KNAcc*100, SVCAcc*100,
                                     NBAcc1*100,  DTAcc*100,
                                     RFAcc*100, GBAcc*100]})
compare.sort_values(by='Accuracy', ascending=False)